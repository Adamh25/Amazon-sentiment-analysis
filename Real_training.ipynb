{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSYGtlSa92aievBJ3OPBO5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adamh25/Amazon-sentiment-analysis/blob/main/Real_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchio -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndIkJ-fShiYW",
        "outputId": "08625329-9c75-4e3b-b0b7-640540418a88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Mount Google Drive and set project paths ---\n",
        "from google.colab import drive\n",
        "import os, glob\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Find your LIFE363_CEREBRO project folder\n",
        "ROOT = \"/content/drive/MyDrive/LIFE363_CEREBRO\"\n",
        "assert os.path.isdir(ROOT), f\"❌ Folder not found: {ROOT}\"\n",
        "\n",
        "# COSTA dataset inside it\n",
        "COSTA = os.path.join(ROOT, \"COSTA-Dataset-v1\")\n",
        "IMG_DIR = os.path.join(COSTA, \"imagesTr\")\n",
        "LBL_DIR = os.path.join(COSTA, \"labelsTr\")\n",
        "\n",
        "print(\"Project root:\", ROOT)\n",
        "print(\"imagesTr:\", IMG_DIR)\n",
        "print(\"labelsTr:\", LBL_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zyu0FN6iJLl",
        "outputId": "96febb33-8965-4def-e486-0f5ce6f87a7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Project root: /content/drive/MyDrive/LIFE363_CEREBRO\n",
            "imagesTr: /content/drive/MyDrive/LIFE363_CEREBRO/COSTA-Dataset-v1/imagesTr\n",
            "labelsTr: /content/drive/MyDrive/LIFE363_CEREBRO/COSTA-Dataset-v1/labelsTr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoJjoxwGhBBt",
        "outputId": "a12ad40f-4c75-4d6b-974c-ad35be240a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing 224 subjects → /content/drive/MyDrive/LIFE363_CEREBRO/data_preprocessed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 224/224 [33:00<00:00,  8.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done: 224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os, glob, numpy as np, nibabel as nib\n",
        "from tqdm import tqdm\n",
        "import torchio as tio  # only for convenient resampling\n",
        "\n",
        "DIR_OUT = os.path.join(ROOT, \"data_preprocessed\")\n",
        "os.makedirs(DIR_OUT, exist_ok=True)\n",
        "\n",
        "TARGET_SPACING = (0.46875, 0.46875, 0.8)  # your ≈[0.47,0.47,0.80]\n",
        "PCT = (1, 99)  # clipping percentiles\n",
        "\n",
        "def clip_zscore(vol, p1=1, p99=99):\n",
        "    lo, hi = np.percentile(vol, [p1, p99])\n",
        "    vol = np.clip(vol, lo, hi)\n",
        "    mu, sd = vol.mean(), vol.std() + 1e-8\n",
        "    return (vol - mu) / sd\n",
        "\n",
        "def save_preprocessed(img_path, lbl_path):\n",
        "    # derive centre and basename\n",
        "    centre = os.path.basename(os.path.dirname(img_path))\n",
        "    base = os.path.basename(img_path).replace(\".nii.gz\",\"\")\n",
        "\n",
        "    # load\n",
        "    I = nib.load(img_path); L = nib.load(lbl_path)\n",
        "    I_np = I.get_fdata().astype(np.float32)\n",
        "    L_np = L.get_fdata().astype(np.uint8)\n",
        "    affine = I.affine\n",
        "\n",
        "    # intensity preprocessing\n",
        "    I_np = clip_zscore(I_np, *PCT)\n",
        "\n",
        "    # build TorchIO images for resampling (uses spacing from headers)\n",
        "    img_t = tio.ScalarImage(tensor=I_np[None, ...], affine=affine)\n",
        "    lbl_t = tio.LabelMap(tensor=L_np[None, ...], affine=affine)\n",
        "\n",
        "    # resample\n",
        "    resample = tio.Resample(target=TARGET_SPACING)\n",
        "    img_r = resample(img_t)\n",
        "    lbl_r = resample(lbl_t)\n",
        "\n",
        "    I_out = img_r.tensor.squeeze(0).numpy().astype(np.float32)\n",
        "    L_out = lbl_r.tensor.squeeze(0).numpy().astype(np.uint8)\n",
        "    aff_out = img_r.affine\n",
        "\n",
        "    # write with unified naming\n",
        "    out_img = os.path.join(DIR_OUT, f\"{base}_{centre}_img.nii.gz\")\n",
        "    out_msk = os.path.join(DIR_OUT, f\"{base}_{centre}_mask.nii.gz\")\n",
        "    nib.save(nib.Nifti1Image(I_out, aff_out), out_img)\n",
        "    nib.save(nib.Nifti1Image(L_out, aff_out), out_msk)\n",
        "    return out_img, out_msk\n",
        "\n",
        "# build pairs from imagesTr/labelsTr\n",
        "img_paths = sorted(glob.glob(f\"{IMG_DIR}/**/*.nii.gz\", recursive=True))\n",
        "lbl_paths = sorted(glob.glob(f\"{LBL_DIR}/**/*.nii.gz\", recursive=True))\n",
        "imgs = {os.path.basename(p): p for p in img_paths}\n",
        "lbls = {os.path.basename(p): p for p in lbl_paths}\n",
        "common = sorted(set(imgs) & set(lbls))\n",
        "\n",
        "print(f\"Preprocessing {len(common)} subjects → {DIR_OUT}\")\n",
        "done = 0\n",
        "for name in tqdm(common):\n",
        "    out_img, out_msk = save_preprocessed(imgs[name], lbls[name])\n",
        "    done += 1\n",
        "print(\"Done:\", done)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- installs\n",
        "!pip -q install monai-weekly nibabel torchio scikit-image matplotlib tqdm\n",
        "\n",
        "import os, glob, shutil, random, json, time\n",
        "import numpy as np, torch\n",
        "\n",
        "# --- deterministic-ish\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --- mount Drive robustly\n",
        "from google.colab import drive\n",
        "MOUNT = \"/content/drive\"\n",
        "if not os.path.ismount(MOUNT):\n",
        "    if os.path.isdir(MOUNT) and os.listdir(MOUNT):\n",
        "        shutil.rmtree(MOUNT)\n",
        "    os.makedirs(MOUNT, exist_ok=True)\n",
        "    drive.mount(MOUNT, force_remount=True)\n",
        "\n",
        "# --- find LIFE363 root + data_preprocessed\n",
        "def find_dir(pattern):\n",
        "    hits = glob.glob(f\"{MOUNT}/MyDrive/**/{pattern}\", recursive=True)\n",
        "    if hits: return hits[0]\n",
        "    raise FileNotFoundError(f\"Couldn't find a folder named '{pattern}' under MyDrive.\")\n",
        "\n",
        "try:\n",
        "    ROOT = find_dir(\"LIFE363_CEREBRO\")\n",
        "except:\n",
        "    ROOT = find_dir(\"LIFE363\")\n",
        "\n",
        "try:\n",
        "    DIR_PREP = find_dir(\"data_preprocessed\")\n",
        "except:\n",
        "    # fallback: any folder with 'preprocess' in name\n",
        "    DIR_PREP = glob.glob(f\"{ROOT}/**/*preprocess*\", recursive=True)[0]\n",
        "\n",
        "DIR_CKPT = os.path.join(ROOT, \"checkpoints\"); os.makedirs(DIR_CKPT, exist_ok=True)\n",
        "DIR_LOGS = os.path.join(ROOT, \"logs\"); os.makedirs(DIR_LOGS, exist_ok=True)\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"DIR_PREP:\", DIR_PREP)\n",
        "print(\"DIR_CKPT:\", DIR_CKPT)\n",
        "print(\"DIR_LOGS:\", DIR_LOGS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xIEmYhnp7Pt",
        "outputId": "64947ae8-c21b-47d6-d408-04a4a1db17fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.7 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDevice: cpu\n",
            "ROOT: /content/drive/MyDrive/LIFE363_CEREBRO\n",
            "DIR_PREP: /content/drive/MyDrive/LIFE363_CEREBRO/data_preprocessed\n",
            "DIR_CKPT: /content/drive/MyDrive/LIFE363_CEREBRO/checkpoints\n",
            "DIR_LOGS: /content/drive/MyDrive/LIFE363_CEREBRO/logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find *_img.nii.gz / *_mask.nii.gz recursively\n",
        "imgs  = sorted(glob.glob(f\"{DIR_PREP}/**/*_img.nii.gz\",  recursive=True))\n",
        "masks = sorted(glob.glob(f\"{DIR_PREP}/**/*_mask.nii.gz\", recursive=True))\n",
        "\n",
        "# Pair by common stem up to _img/_mask\n",
        "def stem(p):\n",
        "    b = os.path.basename(p)\n",
        "    return b.replace(\"_img.nii.gz\",\"\").replace(\"_mask.nii.gz\",\"\")\n",
        "\n",
        "by_stem = {}\n",
        "for p in imgs:  by_stem.setdefault(stem(p), {})[\"img\"]  = p\n",
        "for p in masks: by_stem.setdefault(stem(p), {})[\"mask\"] = p\n",
        "\n",
        "pairs = [{\"img\": v[\"img\"], \"mask\": v[\"mask\"]} for k,v in by_stem.items() if \"img\" in v and \"mask\" in v]\n",
        "print(f\"Found {len(pairs)} image–mask pairs\")\n",
        "if len(pairs) < 2:\n",
        "    print(\"⚠️ Need ≥2 for a split; proceed anyway but validation will be minimal.\")\n",
        "\n",
        "# Simple split: last N as validation (site/subject-wise split comes later)\n",
        "VAL_COUNT = max(1, len(pairs)//5)  # ~20% val\n",
        "train_files = pairs[:-VAL_COUNT] if len(pairs) > 1 else pairs\n",
        "val_files   = pairs[-VAL_COUNT:]   if len(pairs) > 1 else pairs\n",
        "\n",
        "print(f\"Train: {len(train_files)} | Val: {len(val_files)}\")\n",
        "for d in val_files[:3]:\n",
        "    print(\"VAL →\", os.path.basename(d[\"img\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUZyx_RdqQCN",
        "outputId": "5ce57499-8c87-457b-de70-fa8438c2f901"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 226 image–mask pairs\n",
            "Train: 181 | Val: 45\n",
            "VAL → IXI426-IOP-1011-MRA_IXI-IOP_IXI-IOP_img.nii.gz\n",
            "VAL → IXI430-IOP-0990-MRA_IXI-IOP_IXI-IOP_img.nii.gz\n",
            "VAL → IXI462-IOP-1042-MRA_IXI-IOP_IXI-IOP_img.nii.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, RandCropByPosNegLabeld,\n",
        "    RandFlipd, RandRotate90d, ToTensord, Compose\n",
        ")\n",
        "from monai.data import CacheDataset, DataLoader\n",
        "\n",
        "PATCH = (128,128,64)\n",
        "PATCHES_PER_EPOCH = 256  # increase as you add data (e.g., 512+)\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_t = Compose([\n",
        "    LoadImaged(keys=[\"img\",\"mask\"]),\n",
        "    EnsureChannelFirstd(keys=[\"img\",\"mask\"]),\n",
        "    RandCropByPosNegLabeld(\n",
        "        keys=[\"img\",\"mask\"], label_key=\"mask\", spatial_size=PATCH,\n",
        "        pos=2, neg=1, num_samples=PATCHES_PER_EPOCH, image_key=\"img\", image_threshold=0\n",
        "    ),\n",
        "    RandFlipd(keys=[\"img\",\"mask\"], prob=0.5, spatial_axis=0),\n",
        "    RandFlipd(keys=[\"img\",\"mask\"], prob=0.5, spatial_axis=1),\n",
        "    RandFlipd(keys=[\"img\",\"mask\"], prob=0.5, spatial_axis=2),\n",
        "    RandRotate90d(keys=[\"img\",\"mask\"], prob=0.5, max_k=3),\n",
        "    ToTensord(keys=[\"img\",\"mask\"]),\n",
        "])\n",
        "\n",
        "val_t = Compose([\n",
        "    LoadImaged(keys=[\"img\",\"mask\"]),\n",
        "    EnsureChannelFirstd(keys=[\"img\",\"mask\"]),\n",
        "    ToTensord(keys=[\"img\",\"mask\"]),\n",
        "])\n",
        "\n",
        "train_ds = CacheDataset(train_files, transform=train_t, cache_rate=1.0, num_workers=2)\n",
        "val_ds   = CacheDataset(val_files,   transform=val_t,   cache_rate=1.0, num_workers=2)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=1,      shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "len(train_ds), len(val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "swtWOb8lqXzi",
        "outputId": "d3796e96-a771-4b4e-d9ba-afa2281c6855"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4257484238.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from monai.transforms import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mLoadImaged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnsureChannelFirstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandCropByPosNegLabeld\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mRandFlipd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandRotate90d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToTensord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCacheDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/monai/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_submodules\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# handlers_* have some external decorators the users may not have installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/monai/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcomponent_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponentStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMethodReplacer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRestartGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecate_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeprecatedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecated_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecated_arg_default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRankFilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevenly_divisible_all_gather\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dist_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_list_all_gather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m from .enums import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/monai/utils/deprecate_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_leq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/monai/utils/module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# bundle config system flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}